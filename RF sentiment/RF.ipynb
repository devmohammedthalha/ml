{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names based on the structure of your dataset\n",
    "columns = ['target', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Load your dataset with the correct column names\n",
    "df = pd.read_csv('data_set.csv', encoding='ISO-8859-1', header=None, names=columns)\n",
    "\n",
    "# Check the first few rows to verify\n",
    "print(df.head())\n",
    "\n",
    "# Sample 500 records equally from each class (0, 2, 4)\n",
    "df_sample = df.groupby('target', group_keys=False).apply(lambda x: x.sample(n=30000, random_state=42))\n",
    "\n",
    "# Save the sampled dataset\n",
    "df_sample.to_csv('sampled_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\91877\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\91877\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset (modify path if needed)\n",
    "df = pd.read_csv(\"sampled_dataset.csv\", encoding='latin-1', header=None, usecols=[0, 5], names=['target', 'text'])\n",
    "\n",
    "# Convert sentiment labels (0 = negative, 2 = neutral, 4 = positive)\n",
    "df['target'] = df['target'].replace({0: 'Negative', 2: 'Neutral', 4: 'Positive'})\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters & numbers\n",
    "    text = text.lower().strip()  # Convert to lowercase and trim spaces\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 words\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['target']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForestClassifier utilizing all CPU cores\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m input_text2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvery very dangerous\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Preprocess both input texts\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m input_text1_processed \u001b[38;5;241m=\u001b[39m preprocess_text(input_text1)\n\u001b[0;32m      6\u001b[0m input_text2_processed \u001b[38;5;241m=\u001b[39m preprocess_text(input_text2)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Convert the input texts to numerical features using the same TF-IDF vectorizer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_text' is not defined"
     ]
    }
   ],
   "source": [
    "input_text1 = \"hello im fine\"\n",
    "input_text2 = \"very very dangerous\"\n",
    "\n",
    "# Preprocess both input texts\n",
    "input_text1_processed = preprocess_text(input_text1)\n",
    "input_text2_processed = preprocess_text(input_text2)\n",
    "\n",
    "# Convert the input texts to numerical features using the same TF-IDF vectorizer\n",
    "input_vector1 = vectorizer.transform([input_text1_processed])\n",
    "input_vector2 = vectorizer.transform([input_text2_processed])\n",
    "\n",
    "# Predict the sentiment for both inputs\n",
    "predicted_sentiment1 = clf.predict(input_vector1)\n",
    "predicted_sentiment2 = clf.predict(input_vector2)\n",
    "\n",
    "# Map sentiment values to labels\n",
    "sentiment_map = {\n",
    "    'Negative': 0,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 4\n",
    "}\n",
    "\n",
    "# Get the predicted sentiment labels\n",
    "predicted_label1 = predicted_sentiment1[0]\n",
    "predicted_label2 = predicted_sentiment2[0]\n",
    "\n",
    "# Function to print sentiment with explanation\n",
    "def print_sentiment(input_text, predicted_label):\n",
    "    if predicted_label == 'Negative':\n",
    "        print(f\"The sentiment of the input text '{input_text}' is: Negative (0)\")\n",
    "    elif predicted_label == 'Neutral':\n",
    "        print(f\"The sentiment of the input text '{input_text}' is: Neutral (2)\")\n",
    "    else:\n",
    "        print(f\"The sentiment of the input text '{input_text}' is: Positive (4)\")\n",
    "\n",
    "# Print the result for both input texts\n",
    "print_sentiment(input_text1, predicted_label1)\n",
    "print_sentiment(input_text2, predicted_label2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "8966365e1e1c555ff681e829c08a5e6730b1f6b1b4c2abfa0ea1145e98ff3d77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
