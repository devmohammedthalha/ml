{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   target          id                          date     query  \\\n",
      "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names based on the structure of your dataset\n",
    "columns = ['target', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Load your dataset with the correct column names\n",
    "df = pd.read_csv('data_set.csv', encoding='ISO-8859-1', header=None, names=columns)\n",
    "\n",
    "# Check the first few rows to verify\n",
    "print(df.head())\n",
    "\n",
    "# Sample 500 records equally from each class (0, 2, 4)\n",
    "df_sample = df.groupby('target', group_keys=False).apply(lambda x: x.sample(n=30000, random_state=42))\n",
    "\n",
    "# Save the sampled dataset\n",
    "df_sample.to_csv('sampled_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\cse\\nani\\myenv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\cse\\nani\\myenv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\cse\\nani\\myenv\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\cse\\nani\\myenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\cse\\nani\\myenv\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cse\\nani\\myenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\cse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset (modify path if needed)\n",
    "df = pd.read_csv(\"sampled_dataset.csv\", encoding='latin-1', header=None, usecols=[0, 5], names=['target', 'text'])\n",
    "\n",
    "# Convert sentiment labels (0 = negative, 2 = neutral, 4 = positive)\n",
    "df['target'] = df['target'].replace({0: 'Negative', 2: 'Neutral', 4: 'Positive'})\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters & numbers\n",
    "    text = text.lower().strip()  # Convert to lowercase and trim spaces\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use top 5000 words\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "y = df['target']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForestClassifier utilizing all CPU cores\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10, n_jobs=-1, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the input text 'hello im fine' is: Positive (4)\n"
     ]
    }
   ],
   "source": [
    "input_text1 = \"hello im fine\"\n",
    "input_text2 = \"very very dangerous\"\n",
    "\n",
    "# Preprocess both input texts\n",
    "input_text1_processed = preprocess_text(input_text1)\n",
    "input_text2_processed = preprocess_text(input_text2)\n",
    "\n",
    "# Convert the input texts to numerical features using the same TF-IDF vectorizer\n",
    "input_vector1 = vectorizer.transform([input_text1_processed])\n",
    "input_vector2 = vectorizer.transform([input_text2_processed])\n",
    "\n",
    "# Predict the sentiment for both inputs\n",
    "predicted_sentiment1 = clf.predict(input_vector1)\n",
    "predicted_sentiment2 = clf.predict(input_vector2)\n",
    "\n",
    "# Map sentiment values to labels\n",
    "sentiment_map = {\n",
    "    'Negative': 0,\n",
    "    'Neutral': 2,\n",
    "    'Positive': 4\n",
    "}\n",
    "\n",
    "# Get the predicted sentiment labels\n",
    "predicted_label1 = predicted_sentiment1[0]\n",
    "predicted_label2 = predicted_sentiment2[0]\n",
    "\n",
    "# Function to print sentiment with explanation\n",
    "def print_sentiment(input_text, predicted_label):\n",
    "    if predicted_label == 'Negative':\n",
    "        print(f\"The sentiment of the input text '{input_text}' is: Negative (0)\")\n",
    "    elif predicted_label == 'Neutral':\n",
    "        print(f\"The sentiment of the input text '{input_text}' is: Neutral (2)\")\n",
    "    else:\n",
    "        print(f\"The sentiment of the input text '{input_text}' is: Positive (4)\")\n",
    "\n",
    "# Print the result for both input texts\n",
    "print_sentiment(input_text1, predicted_label1)\n",
    "print_sentiment(input_text2, predicted_label2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8966365e1e1c555ff681e829c08a5e6730b1f6b1b4c2abfa0ea1145e98ff3d77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
